{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c9520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error setting up local transformers mode: No module named 'torch'\n",
      "Consider using HF Inference API or http_api mode instead.\n",
      "Example interactive session. This block runs only if you execute this script directly.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_from_messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 195\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# choose which generate function to call based on MODE\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m MODE == \u001b[33m\"\u001b[39m\u001b[33mtransformers_local\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     gen = \u001b[43mgenerate_from_messages\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m MODE == \u001b[33m\"\u001b[39m\u001b[33mhf_inference\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    197\u001b[39m     gen = hf_generate\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_from_messages' is not defined"
     ]
    }
   ],
   "source": [
    "# Mistral 7B Chatbot Notebook (Jupytext / .py with cells)\n",
    "# %%\n",
    "\"\"\"\n",
    "Notebook: Create a chatbot with Mistral 7B\n",
    "\n",
    "This file is written as a Python script with Jupyter cell markers (\"# %%\") so you can\n",
    "- open it in Jupyter/VSCode\n",
    "- or convert to a real .ipynb using jupytext\n",
    "\n",
    "It provides three main options:\n",
    "1) Run locally with `transformers` (best if you have a GPU and enough VRAM or use 4-bit quantization)\n",
    "2) Use Hugging Face Inference API (fast, requires an API token)\n",
    "3) Fallback simple HTTP API example (for other hosted providers)\n",
    "\n",
    "Read the first cells before running.\n",
    "\"\"\"\n",
    "# %%\n",
    "# INSTALL REQUIRED PACKAGES\n",
    "# Run this cell once in your environment. Uncomment and run in Jupyter.\n",
    "# Note: installing bitsandbytes requires a CUDA-enabled GPU and matching CUDA/toolchain.\n",
    "\n",
    "# !pip install --upgrade pip\n",
    "# !pip install transformers accelerate einops sentencepiece huggingface_hub\n",
    "# !pip install bitsandbytes  # optional: for 8/4-bit quantization (CUDA required)\n",
    "# !pip install torch  # ensure torch is installed (GPU build recommended)\n",
    "\n",
    "# %%\n",
    "# CONFIGURATION: choose mode and model\n",
    "MODE = \"transformers_local\"  # options: \"transformers_local\", \"hf_inference\", \"http_api\"\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"  # recommended instruct variant\n",
    "DEVICE = \"auto\"  # or 0 for first GPU\n",
    "\n",
    "# If using Hugging Face Inference API, set HF_TOKEN environment variable or put token here (not recommended)\n",
    "HF_INFERENCE_TOKEN = None  # or \"hf_...\"\n",
    "\n",
    "# %%\n",
    "# SIMPLE CHAT INTERFACE (shared by modes)\n",
    "from typing import List, Dict\n",
    "\n",
    "class ChatHistory:\n",
    "    def __init__(self, system_prompt: str = \"You are a helpful assistant.\"):\n",
    "        self.system = system_prompt\n",
    "        self.messages = []  # list of (role, content)\n",
    "\n",
    "    def add_user(self, text: str):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": text})\n",
    "\n",
    "    def add_assistant(self, text: str):\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "\n",
    "    def as_messages(self):\n",
    "        # returns a list formatted for instruction-style models\n",
    "        msgs = [{\"role\": \"system\", \"content\": self.system}] + self.messages\n",
    "        return msgs\n",
    "\n",
    "    def short_history(self, max_turns: int = 6):\n",
    "        # keep last N turns (pairs)\n",
    "        if len(self.messages) <= max_turns*2:\n",
    "            return self.as_messages()\n",
    "        # keep system + last max_turns*2 items\n",
    "        return [{\"role\": \"system\", \"content\": self.system}] + self.messages[-max_turns*2:]\n",
    "\n",
    "# %%\n",
    "# OPTION 1: Run locally with Hugging Face `transformers` (recommended if you have GPU & CUDA)\n",
    "if MODE == \"transformers_local\":\n",
    "    try:\n",
    "        import torch\n",
    "        from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        import os\n",
    "\n",
    "        print(\"Using transformers local mode\")\n",
    "\n",
    "        # If you have limited VRAM, you can enable 4-bit quantization (needs bitsandbytes)\n",
    "        use_4bit = True\n",
    "\n",
    "        if use_4bit:\n",
    "            # Adjust these params based on your setup\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "            )\n",
    "            print(\"Loading model with 4-bit quantization (requires bitsandbytes)\")\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                device_map=DEVICE,\n",
    "                quantization_config=bnb_config,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                device_map=DEVICE,\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "\n",
    "        # Create a simple generate helper using model.generate\n",
    "        def generate_from_messages(messages: List[Dict], max_new_tokens: int = 256, temperature: float = 0.2):\n",
    "            # For instruct-style models we concatenate system + messages into a single prompt\n",
    "            prompt = \"\"\n",
    "            # Simple formatting: include roles\n",
    "            for m in messages:\n",
    "                role = m.get(\"role\", \"user\")\n",
    "                content = m.get(\"content\", \"\")\n",
    "                if role == \"system\":\n",
    "                    prompt += f\"<|system|>\" + content + \"\\n\"\n",
    "                elif role == \"user\":\n",
    "                    prompt += f\"<|user|>\" + content + \"\\n\"\n",
    "                else:\n",
    "                    prompt += f\"<|assistant|>\" + content + \"\\n\"\n",
    "\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            decoded = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "            return decoded\n",
    "\n",
    "        # Example interactive loop (run in a cell)\n",
    "        ch = ChatHistory(system_prompt=\"You are a helpful assistant.\")\n",
    "        print(\"Chatbot ready (local). Use ch.add_user('...') and then call generate_from_messages(ch.as_messages())\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error setting up local transformers mode:\", e)\n",
    "        print(\"Consider using HF Inference API or http_api mode instead.\")\n",
    "\n",
    "# %%\n",
    "# OPTION 2: Hugging Face Inference API (no local GPU required)\n",
    "if MODE == \"hf_inference\":\n",
    "    import os, requests, json\n",
    "    HF_TOKEN = HF_INFERENCE_TOKEN or os.environ.get(\"HF_TOKEN\")\n",
    "    if not HF_TOKEN:\n",
    "        raise RuntimeError(\"Set HF_INFERENCE_TOKEN variable or HF_TOKEN env var to use Hugging Face Inference API\")\n",
    "\n",
    "    HF_MODEL = MODEL_ID\n",
    "    HF_API_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "    def hf_generate(messages: List[Dict], max_tokens: int = 256, temperature: float = 0.2):\n",
    "        # For the inference API, we pass the messages in a single string prompt.\n",
    "        prompt = \"\".join([f\"{m['role']}: {m['content']}\\n\" for m in messages])\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": max_tokens, \"temperature\": temperature},\n",
    "        }\n",
    "        resp = requests.post(HF_API_URL, headers=headers, json=payload, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        # API returns generated text in different shapes depending on model; try to extract text\n",
    "        if isinstance(data, list) and \"generated_text\" in data[0]:\n",
    "            return data[0][\"generated_text\"]\n",
    "        elif isinstance(data, dict) and \"generated_text\" in data:\n",
    "            return data[\"generated_text\"]\n",
    "        else:\n",
    "            # fallback: stringify\n",
    "            return str(data)\n",
    "\n",
    "    ch = ChatHistory()\n",
    "    print(\"Hugging Face Inference API helper ready. Use hf_generate(ch.as_messages())\")\n",
    "\n",
    "# %%\n",
    "# OPTION 3: Generic HTTP API (example template for other hosted providers e.g. Clarifai, Replicate)\n",
    "if MODE == \"http_api\":\n",
    "    import requests, os\n",
    "    API_URL = \"https://api.example.com/generate\"  # replace with provider\n",
    "    API_KEY = os.environ.get(\"MY_API_KEY\")\n",
    "\n",
    "    def http_generate(messages: List[Dict], max_tokens: int = 256):\n",
    "        prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "        resp = requests.post(API_URL, json={\"prompt\": prompt, \"max_tokens\": max_tokens}, headers={\"Authorization\": f\"Bearer {API_KEY}\"})\n",
    "        return resp.json()\n",
    "\n",
    "    print(\"HTTP API template ready. Replace API_URL and API_KEY with your provider settings.\")\n",
    "\n",
    "# %%\n",
    "# USAGE EXAMPLE (universal)\n",
    "# After you run the appropriate mode cell, run the following to chat interactively in Jupyter:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Example interactive session. This block runs only if you execute this script directly.\")\n",
    "    # choose which generate function to call based on MODE\n",
    "    if MODE == \"transformers_local\":\n",
    "        gen = generate_from_messages\n",
    "    elif MODE == \"hf_inference\":\n",
    "        gen = hf_generate\n",
    "    else:\n",
    "        gen = lambda msgs, **k: \"Replace with your provider generate function\"\n",
    "\n",
    "    ch = ChatHistory(system_prompt=\"You are a helpful assistant.\")\n",
    "    while True:\n",
    "        u = input(\"You: \")\n",
    "        if u.strip().lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "        ch.add_user(u)\n",
    "        reply = gen(ch.short_history())\n",
    "        print(\"Assistant:\", reply)\n",
    "        ch.add_assistant(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63560b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
